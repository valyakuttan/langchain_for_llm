{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Large Language Models** never remember previous conversions\n",
    "\n",
    "![memory](../images/L2/memory.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **chatbots** store the previous conversations and give the whole\n",
    "  convesation as context to **LLM**\n",
    "\n",
    "![meomory with context](../images/L2/memory_with_context.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `langchain` provides several kinds of memory to store and accumulate\n",
    "   the conversation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of memories\n",
    "\n",
    "![Types of memories](../images/L2/memory_types.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ConversationBufferMemory\n",
    "\n",
    "- Stores the entire conversation\n",
    "- The whole conversation is given as a context to the next prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.1: A chatbot using `langchain`'s `ConversationBufferMemory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hi, my name is Andrew\")\n",
    "\n",
    "\"\"\"\n",
    "> Entering new ConversationChain chain...\n",
    "Prompt after formatting:\n",
    "The following is a friendly conversation between a human and an AI. The AI is \\\n",
    "talkative and provides lots of specific details from its context. If the AI \\\n",
    "does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "\n",
    "Human: Hi, my name is Andrew\n",
    "AI:\n",
    "\n",
    "> Finished chain.\n",
    "\"Hello Andrew, it's nice to meet you. My name is AI. How can I assist you today?\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")\n",
    "\n",
    "\"\"\"\n",
    "> Entering new ConversationChain chain...\n",
    "Prompt after formatting:\n",
    "The following is a friendly conversation between a human and an AI. \\\n",
    "The AI is talkative and provides lots of specific details from its \\\n",
    "context. If the AI does not know the answer to a question, it \\\n",
    "truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "Human: Hi, my name is Andrew\n",
    "AI: Hello Andrew, it's nice to meet you. My name is AI. How can I assist you today?\n",
    "Human: What is 1+1?\n",
    "AI:\n",
    "\n",
    "> Finished chain.\n",
    "'The answer to 1+1 is 2.'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What is my name?\")\n",
    "\n",
    "\"\"\"\n",
    "> Entering new ConversationChain chain...\n",
    "Prompt after formatting:\n",
    "The following is a friendly conversation between a human and an AI. \\\n",
    "The AI is talkative and provides lots of specific details from \\\n",
    "its context. If the AI does not know the answer to a question, \\\n",
    "it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "Human: Hi, my name is Andrew\n",
    "AI: Hello Andrew, it's nice to meet you. My name is AI. How can I assist you today?\n",
    "Human: What is 1+1?\n",
    "AI: The answer to 1+1 is 2.\n",
    "Human: What is my name?\n",
    "AI:\n",
    "\n",
    "> Finished chain.\n",
    "'Your name is Andrew, as you mentioned earlier.'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory.buffer)\n",
    "\n",
    "\"\"\"\n",
    "Human: Hi, my name is Andrew\n",
    "AI: Hello Andrew, it's nice to meet you. My name is AI. How can I assist you today?\n",
    "Human: What is 1+1?\n",
    "AI: The answer to 1+1 is 2.\n",
    "Human: What is my name?\n",
    "AI: Your name is Andrew, as you mentioned earlier.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})\n",
    "\n",
    "\"\"\"\n",
    "{'history': \"Human: Hi, my name is Andrew\\nAI: Hello Andrew, \\\n",
    "            it's nice to meet you. My name is AI. How can I \\\n",
    "            assist you today?\\nHuman: What is 1+1?\\nAI: The \\\n",
    "            answer to 1+1 is 2.\\nHuman: What is my name?\\nAI: \\\n",
    "            Your name is Andrew, as you mentioned earlier.\"\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory.buffer)\n",
    "\n",
    "\"\"\"\n",
    "Human: Hi\n",
    "AI: What's up\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})\n",
    "\n",
    "\"\"\"\n",
    "{'history': \"Human: Hi\\nAI: What's up\"}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})\n",
    "\n",
    "\"\"\"\n",
    "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ConversationBufferWindowMemory\n",
    "\n",
    "- This will only keep a window of memory\n",
    "\n",
    "- the size of the window can be set by the user"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.1: A chatbot using `langchain`'s `ConversationBufferWindowMemory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})\n",
    "\n",
    "# Note: only the last conversion is remembered\n",
    "\n",
    "\"\"\"\n",
    "{'history': 'Human: Not much, just hanging\\nAI: Cool'}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0)\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hi, my name is Andrew\")\n",
    "\n",
    "\"\"\"\n",
    "> Entering new ConversationChain chain...\n",
    "Prompt after formatting:\n",
    "The following is a friendly conversation between a human and an AI. \\\n",
    "The AI is talkative and provides lots of specific details from \\\n",
    "its context. If the AI does not know the answer to a question, \\\n",
    "it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "\n",
    "Human: Hi, my name is Andrew\n",
    "AI:\n",
    "\n",
    "> Finished chain.\n",
    "\"Hello Andrew, it's nice to meet you. My name is AI. How can I assist you today?\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")\n",
    "\n",
    "\"\"\"\n",
    "> Entering new ConversationChain chain...\n",
    "Prompt after formatting:\n",
    "The following is a friendly conversation between a human and an AI. \\\n",
    "The AI is talkative and provides lots of specific details from its \\\n",
    "context. If the AI does not know the answer to a question, it \\\n",
    "truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "Human: Hi, my name is Andrew\n",
    "AI: Hello Andrew, it's nice to meet you. My name is AI. How can I assist you today?\n",
    "Human: What is 1+1?\n",
    "AI:\n",
    "\n",
    "> Finished chain.\n",
    "'The answer to 1+1 is 2.'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What is my name?\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "> Entering new ConversationChain chain...\n",
    "Prompt after formatting:\n",
    "The following is a friendly conversation between a human and an \\\n",
    "AI. The AI is talkative and provides lots of specific details from its \\\n",
    "context. If the AI does not know the answer to a question, it \\\n",
    "truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "Human: What is 1+1?\n",
    "AI: The answer to 1+1 is 2.\n",
    "Human: What is my name?\n",
    "AI:\n",
    "\n",
    "> Finished chain.\n",
    "\"I'm sorry, I don't have access to that information. Could you please tell me your name?\"\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ConversationTokenBufferMemory\n",
    "\n",
    "- This will limit the number of tokens saved\n",
    "\n",
    "- Since many LLMs charge based on tokens this maps directly\n",
    "  the const of LLM calls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.1: A chatbot using `langchain`'s `ConversationTokenBufferMemory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "llm = ChatOpenAI(temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm is required to create ConversationTokenBufferMemory becase different\n",
    "# llm's count tokens differently\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=30)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})\n",
    "\n",
    "\"\"\"\n",
    "{'history': 'AI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ConversationSummaryMemory\n",
    "\n",
    "- Here istead of limiting the stored conversation, we use an LLM\n",
    "  to summarize the whole conversation and store that"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4.1: A chatbot using `langchain`'s `ConversationSummaryBufferMemory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})\n",
    "\n",
    "\"\"\"\n",
    "{'history': \"System: The human and AI engage in small talk before discussing \\\n",
    "            the day's schedule. The AI informs the human of a morning meeting \\\n",
    "            with the product team, time to work on the LangChain project, and \\\n",
    "            a lunch meeting with a customer interested in the latest AI developments.\"}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")\n",
    "\n",
    "\"\"\"\n",
    "> Entering new ConversationChain chain...\n",
    "Prompt after formatting:\n",
    "The following is a friendly conversation between a human and an AI. \\\n",
    "The AI is talkative and provides lots of specific details from its \\\n",
    "context. If the AI does not know the answer to a question, it truthfully \\\n",
    "says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "System: The human and AI engage in small talk before discussing the day's \\\n",
    "schedule. The AI informs the human of a morning meeting with the product team, \\\n",
    "time to work on the LangChain project, and a lunch meeting with a customer \\\n",
    "interested in the latest AI developments.\n",
    "Human: What would be a good demo to show?\n",
    "AI:\n",
    "\n",
    "> Finished chain.\n",
    "\"Based on the customer's interest in AI developments, I would suggest showcasing \\\n",
    "our latest natural language processing capabilities. We could demonstrate how our AI \\\n",
    "can accurately understand and respond to complex language queries, and even provide \\\n",
    "personalized recommendations based on the user's preferences. Additionally, we could \\\n",
    "highlight our AI's ability to learn and adapt over time, making it a valuable tool for \\\n",
    "businesses looking to improve their customer experience.\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})\n",
    "\n",
    "# Note: memory stores recent conversation upto the maximum tokens.\n",
    "# Everything else is incorporated into the summary.\n",
    "\n",
    "\"\"\"\n",
    "{'history': \"System: The human and AI engage in small talk before discussing \\\n",
    "            the day's schedule. The AI informs the human of a morning meeting \\\n",
    "            with the product team, time to work on the LangChain project, and \\\n",
    "            a lunch meeting with a customer interested in the latest AI \\\n",
    "            developments. The human asks what would be a good demo to show.\\nAI: \\\n",
    "            Based on the customer's interest in AI developments, I would suggest \\\n",
    "            showcasing our latest natural language processing capabilities. We \\\n",
    "            could demonstrate how our AI can accurately understand and respond to \\\n",
    "            complex language queries, and even provide personalized recommendations \\\n",
    "            based on the user's preferences. Additionally, we could highlight our \\\n",
    "            AI's ability to learn and adapt over time, making it a valuable tool for \\\n",
    "            businesses looking to improve their customer experience.\"\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Memory Types\n",
    "\n",
    "![Additional Memories](../images/L2/additional_memory_types.png)\n",
    "\n",
    "- **Vector Data Memory** stores **word embeddings** and **vector embeddings**.\n",
    "\n",
    "- **Entity Memory** can be use to remember facts about an **entity**, say a person."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain_For_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
