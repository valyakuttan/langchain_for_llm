{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain: Q&A over Documents\n",
    "\n",
    "An example might be a tool that would allow you to query a product catalog for items of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U docarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving answers form documents using langchain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../OutdoorClothingCatalog_1000.csv'\n",
    "loader = CSVLoader(file_path=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import DocArrayInMemorySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch \n",
    ").from_loaders([loader])\n",
    "\n",
    "# Note: OpenAIEmbeddings is the default embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =  \"Please list all your shirts with sun protection in a table \\\n",
    "in markdown and summarize each one.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = index.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's happening underneath"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM's on Documents\n",
    "![LLM intro](../images/llm_intro.png)\n",
    "  - LLM's can inspect only a few thousand words at a time\n",
    "  - If we have large documents how can we get LLM's to answer quesion about it\n",
    "  - This is where **vector stores** and **embeddings** come into play"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "![Embeddings](../images/embeddings.png)\n",
    "  - Embedding creates numerical representaion for pieces of text\n",
    "  - This numerical representaion captures semantic meaning of the piece of text\n",
    "  - Text with similar contents have similar vectors\n",
    "  - This allows us to compare pieces of text in the vector space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Consider the exapmple below\n",
    "  1. My dog Rover likes to chase squirrels.\n",
    "  1. Fluffy, my cat, refuses to eat from a can.\n",
    "  1. The Chevy Bolt accelerates to 60 mph in 6.7 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"My dog Rover likes to chase squirrels.\"\n",
    "text2 = \"Fluffy, my cat, refuses to eat from a can.\"\n",
    "text3 = \"The Chevy Bolt accelerates to 60 mph in 6.7 seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed1 = embeddings.embed_query(text1)\n",
    "embed2 = embeddings.embed_query(text2)\n",
    "embed3 = embeddings.embed_query(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(embed1)) \n",
    "# 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(embed1[:5])\n",
    "# [-0.02120191603899002, -0.010773591697216034, 0.004235907457768917, 0.006397020071744919, -0.024629006162285805]\n",
    "\n",
    "# print(embed2[:5])\n",
    "# [-0.02368231862783432, -0.027932319790124893, -0.006375002674758434, -0.002569616539403796, -0.0007376205176115036]\n",
    "\n",
    "# print(embed3[:5])\n",
    "# [0.004903301130980253, 0.014738245867192745, 0.002021784894168377, -0.013944647274911404, -0.022346707060933113]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If you look at the vector space\n",
    "  - Sentences 1 and 2 are very similar because they refers to pets.\n",
    "  - But sentence 3 is not similar to either 1 or 2, since it's about a car."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hence\n",
    "- This allows us to easily figure out which pieces of texts are similar.\n",
    "- So we can choose which pieces of text to be pass into the LLM to answer question."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Database\n",
    "\n",
    " ![Vector Database](../images/vector_databse.png)\n",
    "\n",
    " - Stores the vectors generated by embeddings\n",
    " - We break large documents into smaller chunks and store them along with\n",
    "   their embeddings.\n",
    " - This is what happens when we create an index.\n",
    " - When a query comes we first create an embedding for that query\n",
    " - Then we compare with all vectors in the vector database and we pick\n",
    "   the most similar n chunks.\n",
    "\n",
    " ![Vector index](../images/vector_index.png)\n",
    "\n",
    " - These are then passed in the prompt to the language model to\n",
    "   get back the final answer. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A detailed step by step way to get answers from documents using langchain\n",
    "\n",
    " - This will enble us to learn what's happening under the hood"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a document loader from the csv with all discriptions of the products that we want to do question answering over "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "file = '../OutdoorClothingCatalog_1000.csv'\n",
    "loader = CSVLoader(file_path=file)\n",
    "docs = loader.load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create embeddings for all pieces of text and store them in a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import DocArrayInMemorySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    docs, \n",
    "    embeddings\n",
    ")\n",
    "\n",
    "# this will create a vector store"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 We can query the vector store to get entries similar to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Please suggest a shirt with sunblocking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevent_docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(relevent_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relevent_docs[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. How do we use this vector store to perform Quesion Answering over the documents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 We create a retriever\n",
    " - A retriever is a genric interface that can be underpinned by\n",
    "   any method that takes a query and returns documents\n",
    "   \n",
    " - vector store and embeddings are one such method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Since we want text generation and a natural language response we need a language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature = 0.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Answers to a question over the documents can be obtained by\n",
    "\n",
    " - Combine all documents into a single piece of text\n",
    "\n",
    " - pass the text along with the question to the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdocs = \"\".join([relevent_docs[i].page_content for i in range(len(relevent_docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.call_as_llm(f\"{qdocs} Question: Please list all your \\\n",
    "shirts with sun protection in a table in markdown and summarize each one.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the above steps can be encapsulated by the langchain chain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A `RetrievalQA` chain does retrieval and Q&A over the retrieved documents.\n",
    "\n",
    "#### To create a `RetrievalQA` chain we need to pass\n",
    "\n",
    " - A language model for text generation\n",
    "\n",
    " - A chain type, a method which determines how the context is generated\n",
    "   and how to call language model for the answer. In `stuff` method all documents are stuffed to the context and make one call to the language\n",
    "   model\n",
    "\n",
    " - A retriever to fetch documents and pass it to the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA # this will retrieve answers from given documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can create a query and run the chain on this query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =  \"Please list all your shirts with sun protection in a table \\\n",
    "in markdown and summarize each one.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = qa_stuff.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can do retrieval from customized indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch, # customized vectorstore\n",
    "    embedding=embeddings, # customized embeddings\n",
    ").from_loaders([loader])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stuff Method\n",
    "\n",
    "![Stuff Method](../images/suff_method.png)\n",
    "\n",
    " - Simple but unsutable for large documents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map_reduce\n",
    "\n",
    "![Map_reduce](../images/map_reduce.png)\n",
    "\n",
    "- Take all the chunks and passes them along with the question to llm\n",
    "\n",
    "- Get the response and use another llm to summarize all the individual\n",
    "  response to a final answer\n",
    "\n",
    "- Here we can do individual queries in parallel\n",
    "\n",
    "- But takes more calls\n",
    "\n",
    "- Treats all documents independent, which may not always be desirable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refine\n",
    "\n",
    "![Refine](../images/refine.png)\n",
    "\n",
    "- It loops over many documents\n",
    "\n",
    "- But unlike map_reduce it does it iteratively, it builds up on the answer\n",
    "  from the previos documents\n",
    "\n",
    "- Good for combining information and building up an answer over time\n",
    "\n",
    "- Generally lead to longer answers\n",
    "\n",
    "- Not very fast, bceause call are done iteratively"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map_rerank\n",
    "\n",
    "![Map_rerank](../images/map_rerank.png)\n",
    "\n",
    " - Do a single call to a language model for each document and ask it\n",
    "   to return a score.\n",
    "\n",
    " - This relies on language model to know what the score should be\n",
    "\n",
    " - Select the documents with highest score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain_For_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
